"""
Model EÄŸitim ModÃ¼lÃ¼ â€” NetShield-AI (IDS)
==========================================
Random Forest sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± ile aÄŸ trafiÄŸi anomali tespiti.
Confusion Matrix, Precision, Recall, F1-Score deÄŸerlerini hesaplar.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DENETÄ°MLÄ° (SUPERVISED) vs DENETÄ°MSÄ°Z (UNSUPERVISED) Ã–ÄRENME
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Bu IDS projesi iÃ§in iki temel yaklaÅŸÄ±m deÄŸerlendirilebilir:

â”Œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”
â”‚                  DENETÄ°MLÄ° Ã–ÄRENME (Supervised)                  â”‚
â”‚                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚  Algoritma: Random Forest, SVM, XGBoost                         â”‚
â”‚                                                                   â”‚
â”‚  âœ… Avantajlar:                                                  â”‚
â”‚    â€¢ Bilinen saldÄ±rÄ± tÃ¼rlerini Ã§ok yÃ¼ksek doÄŸrulukla tespit ederâ”‚
â”‚    â€¢ SÄ±nÄ±flandÄ±rma metrikleri (Precision, Recall) gÃ¼venilir     â”‚
â”‚    â€¢ Etiketli veri varsa en iyi performansÄ± verir               â”‚
â”‚    â€¢ SaldÄ±rÄ± tÃ¼rlerini kategorize edebilir (DoS, Probe, R2L)    â”‚
â”‚                                                                   â”‚
â”‚  âŒ Dezavantajlar:                                               â”‚
â”‚    â€¢ Etiketli (labeled) veri gerektirir â€” pahalÄ± ve zaman alÄ±cÄ± â”‚
â”‚    â€¢ Yeni/bilinmeyen saldÄ±rÄ± tÃ¼rlerini (zero-day) tanÄ±yamaz     â”‚
â”‚    â€¢ EÄŸitim verisindeki daÄŸÄ±lÄ±ma baÄŸÄ±mlÄ±dÄ±r                     â”‚
â”œâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”¤
â”‚                 DENETÄ°MSÄ°Z Ã–ÄRENME (Unsupervised)                â”‚
â”‚                 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚  Algoritma: Isolation Forest, One-Class SVM, Autoencoder        â”‚
â”‚                                                                   â”‚
â”‚  âœ… Avantajlar:                                                  â”‚
â”‚    â€¢ Etiketli veriye ihtiyaÃ§ duymaz â€” sadece "normal" veri yeterâ”‚
â”‚    â€¢ Zero-day saldÄ±rÄ±larÄ± tespit edebilir (anomali = bilinmeyen)â”‚
â”‚    â€¢ Daha az veri toplama maliyeti                               â”‚
â”‚                                                                   â”‚
â”‚  âŒ Dezavantajlar:                                               â”‚
â”‚    â€¢ Daha yÃ¼ksek False Positive oranÄ± (normal trafiÄŸi saldÄ±rÄ±   â”‚
â”‚      olarak iÅŸaretleyebilir)                                     â”‚
â”‚    â€¢ SaldÄ±rÄ± tÃ¼rÃ¼nÃ¼ belirleyemez, sadece "anomali" der          â”‚
â”‚    â€¢ EÅŸik deÄŸeri (threshold) ayarÄ± zordur                        â”‚
â””â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”˜

BU PROJEDE:
  NSL-KDD veri seti ETÄ°KETLÄ° olduÄŸu iÃ§in DENETÄ°MLÄ° Ã–ÄRENME kullanÄ±yoruz.
  Random Forest tercih edildi Ã§Ã¼nkÃ¼:
  1. Tabular veriler iÃ§in en gÃ¼venilir algoritmalardan biridir
  2. Overfitting'e direnÃ§lidir (bagging ensemble)
  3. Feature importance verdiÄŸi iÃ§in hangi trafik Ã¶zelliklerinin
     saldÄ±rÄ± tespitinde Ã¶nemli olduÄŸunu gÃ¶sterir
  4. Hem bÃ¼yÃ¼k hem kÃ¼Ã§Ã¼k veri setlerinde iyi Ã§alÄ±ÅŸÄ±r
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""

import os
import sys
import time
import warnings
import numpy as np
import joblib
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc
)
from sklearn.model_selection import cross_val_score

warnings.filterwarnings('ignore')

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

from src.preprocessing import load_data, preprocess

FIGURES_DIR = os.path.join(PROJECT_ROOT, 'reports', 'figures')


def plot_confusion_matrix(y_true, y_pred, model_name='Random Forest'):
    """Confusion Matrix gÃ¶rselleÅŸtirmesi."""
    os.makedirs(FIGURES_DIR, exist_ok=True)
    cm = confusion_matrix(y_true, y_pred)

    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=['Normal', 'Attack'],
        yticklabels=['Normal', 'Attack'],
        ax=ax, linewidths=0.5
    )
    ax.set_xlabel('Predicted', fontsize=12)
    ax.set_ylabel('Actual', fontsize=12)
    ax.set_title(f'Confusion Matrix â€” {model_name}', fontsize=14, fontweight='bold')

    # â”€â”€ False Negative AÃ§Ä±klamasÄ± â”€â”€
    # Confusion Matrix'teki 4 hÃ¼cre:
    #   TN (True Negative)  : Normal trafiÄŸi doÄŸru tespit
    #   FP (False Positive)  : Normal trafiÄŸi yanlÄ±ÅŸlÄ±kla saldÄ±rÄ± olarak iÅŸaretleme
    #   FN (False Negative) : SALDIRIYI KAÃ‡IRMA â€” EN TEHLÄ°KELÄ° DURUM!
    #   TP (True Positive)  : SaldÄ±rÄ±yÄ± doÄŸru tespit
    fn = cm[1][0]  # GerÃ§ek saldÄ±rÄ± ama Normal diye tahmin edilen
    ax.text(0.5, -0.15,
            f'âš ï¸ False Negative (KaÃ§Ä±rÄ±lan SaldÄ±rÄ±): {fn}',
            transform=ax.transAxes, fontsize=10, ha='center',
            color='red', fontweight='bold')

    plt.tight_layout()
    path = os.path.join(FIGURES_DIR, f'confusion_matrix_{model_name.lower().replace(" ", "_")}.png')
    fig.savefig(path, dpi=150, bbox_inches='tight')
    plt.close(fig)
    print(f"  [âœ“] Kaydedildi: {path}")
    return cm


def plot_roc_curve(y_true, y_proba, model_name='Random Forest'):
    """ROC Curve gÃ¶rselleÅŸtirmesi."""
    os.makedirs(FIGURES_DIR, exist_ok=True)
    fpr, tpr, _ = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr, tpr)

    fig, ax = plt.subplots(figsize=(8, 6))
    ax.plot(fpr, tpr, color='#2196F3', lw=2.5, label=f'{model_name} (AUC = {roc_auc:.4f})')
    ax.plot([0, 1], [0, 1], color='gray', lw=1.5, linestyle='--')
    ax.fill_between(fpr, tpr, alpha=0.1, color='#2196F3')
    ax.set_xlabel('False Positive Rate', fontsize=12)
    ax.set_ylabel('True Positive Rate', fontsize=12)
    ax.set_title(f'ROC Curve â€” {model_name}', fontsize=14, fontweight='bold')
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    path = os.path.join(FIGURES_DIR, f'roc_curve_{model_name.lower().replace(" ", "_")}.png')
    fig.savefig(path, dpi=150, bbox_inches='tight')
    plt.close(fig)
    print(f"  [âœ“] Kaydedildi: {path}")
    return roc_auc


def plot_feature_importance(model, feature_names, model_name='Random Forest', top_n=20):
    """En Ã¶nemli N feature'Ä± gÃ¶rselleÅŸtirir."""
    os.makedirs(FIGURES_DIR, exist_ok=True)

    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1][:top_n]

    fig, ax = plt.subplots(figsize=(10, 8))
    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, top_n))
    ax.barh(
        range(top_n),
        importances[indices][::-1],
        color=colors[::-1], edgecolor='white'
    )
    ax.set_yticks(range(top_n))
    ax.set_yticklabels([feature_names[i] for i in indices][::-1], fontsize=9)
    ax.set_xlabel('Importance', fontsize=12)
    ax.set_title(f'Top {top_n} Feature Importance â€” {model_name}', fontsize=14, fontweight='bold')
    ax.grid(True, axis='x', alpha=0.3)

    plt.tight_layout()
    path = os.path.join(FIGURES_DIR, f'feature_importance_{model_name.lower().replace(" ", "_")}.png')
    fig.savefig(path, dpi=150, bbox_inches='tight')
    plt.close(fig)
    print(f"  [âœ“] Kaydedildi: {path}")


def train_random_forest(X_train, X_test, y_train, y_test, feature_names):
    """
    Random Forest sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±nÄ± eÄŸitir ve tÃ¼m metrikleri hesaplar.

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    SÄ°BER GÃœVENLÄ°KTE FALSE NEGATIVE'Ä°N KRÄ°TÄ°K Ã–NEMÄ°
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    False Negative (FN) = GerÃ§ek bir saldÄ±rÄ±yÄ± "Normal" olarak
    yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmak.

    Bu, siber gÃ¼venlikte EN TEHLÄ°KELÄ° durumdur Ã§Ã¼nkÃ¼:

    1. SALDIRI TESPÄ°T EDÄ°LEMEZ: SaldÄ±rgan aÄŸda fark edilmeden
       hareket eder, veri Ã§alar veya sisteme zarar verir.

    2. GEÃ‡ KALMA: SaldÄ±rÄ± ancak hasar oluÅŸtuktan sonra fark edilir.
       IBM raporuna gÃ¶re ortalama tespit sÃ¼resi 277 gÃ¼n!

    3. MADDÄ° KAYIP: Bir veri ihlalinin ortalama maliyeti $4.45M (2023).

    False Positive (FP) = Normal trafiÄŸi "SaldÄ±rÄ±" olarak iÅŸaretlemek.
    Bu da kÃ¶tÃ¼dÃ¼r (alarm yorgunluÄŸu) ama FN kadar tehlikeli DEÄÄ°LDÄ°R.

    SONUÃ‡: IDS sistemlerinde RECALL (True Positive Rate) metriÄŸi
    en Ã¶nemli metriktir Ã§Ã¼nkÃ¼:
      Recall = TP / (TP + FN)
    YÃ¼ksek Recall = DÃ¼ÅŸÃ¼k FN = Daha az kaÃ§Ä±rÄ±lan saldÄ±rÄ±.

    Bu yÃ¼zden modeli optimize ederken Accuracy'den Ã§ok RECALL'a
    odaklanmalÄ±yÄ±z!
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ RANDOM FOREST SINIFLANDIRICI EÄÄ°TÄ°MÄ°")
    print("=" * 60)

    # Model oluÅŸtur
    # n_estimators=200: 200 karar aÄŸacÄ± kullanÄ±lÄ±r (ensemble)
    # max_depth=20: Her aÄŸacÄ±n maksimum derinliÄŸi (overfitting kontrolÃ¼)
    # n_jobs=-1: TÃ¼m CPU Ã§ekirdeklerini kullan (paralel eÄŸitim)
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1,
    )

    # â”€â”€ Model EÄŸitimi â”€â”€
    # EÄŸitim verisi (%80) ile model eÄŸitilir
    start = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start
    print(f"  EÄŸitim sÃ¼resi: {train_time:.2f}s")

    # â”€â”€ Tahmin â”€â”€
    # Test verisi (%20) Ã¼zerinde tahmin yapÄ±lÄ±r
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]  # SaldÄ±rÄ± olasÄ±lÄ±ÄŸÄ±

    # â”€â”€ Performans Metrikleri â”€â”€
    # Sadece accuracy yetmez! Siber gÃ¼venlikte tÃ¼m metrikler Ã¶nemlidir:
    acc = accuracy_score(y_test, y_pred)       # DoÄŸruluk
    prec = precision_score(y_test, y_pred)     # Precision: TP / (TP + FP)
    rec = recall_score(y_test, y_pred)         # Recall: TP / (TP + FN) â† EN Ã–NEMLÄ°!
    f1 = f1_score(y_test, y_pred)              # F1: Precision ve Recall'Ä±n harmonik ortalamasÄ±

    print(f"\n  ğŸ“Š Performans Metrikleri:")
    print(f"  {'â”€' * 35}")
    print(f"  Accuracy:    {acc:.4f}  ({acc*100:.2f}%)")
    print(f"  Precision:   {prec:.4f}  ({prec*100:.2f}%)")
    print(f"  Recall:      {rec:.4f}  ({rec*100:.2f}%)")  # En Ã¶nemli metrik!
    print(f"  F1-Score:    {f1:.4f}  ({f1*100:.2f}%)")

    # â”€â”€ Cross-Validation â”€â”€
    # 5-Fold CV ile modelin genelleÅŸtirme yeteneÄŸini test ediyoruz
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')
    print(f"  CV Recall:   {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")

    # â”€â”€ Classification Report â”€â”€
    # Her sÄ±nÄ±f iÃ§in ayrÄ± ayrÄ± metrikler
    print(f"\n  ğŸ“‹ Classification Report:")
    print("  " + "â”€" * 50)
    report = classification_report(y_test, y_pred, target_names=['Normal', 'Attack'])
    for line in report.split('\n'):
        print(f"  {line}")

    # â”€â”€ Confusion Matrix â”€â”€
    # Confusion Matrix'i hem yazdÄ±r hem gÃ¶rselleÅŸtir
    cm = plot_confusion_matrix(y_test, y_pred, 'Random Forest')
    tn, fp, fn, tp = cm.ravel()

    print(f"\n  ğŸ“ Confusion Matrix DetaylarÄ±:")
    print(f"  {'â”€' * 40}")
    print(f"  True Negative  (TN): {tn:>5} â€” Normal trafiÄŸi doÄŸru tespit")
    print(f"  False Positive (FP): {fp:>5} â€” Normal ama saldÄ±rÄ± dedi (yanlÄ±ÅŸ alarm)")
    print(f"  False Negative (FN): {fn:>5} â€” SALDIRIYI KAÃ‡IRDI! âš ï¸")
    print(f"  True Positive  (TP): {tp:>5} â€” SaldÄ±rÄ±yÄ± doÄŸru tespit âœ…")

    # âš ï¸ SÄ°BER GÃœVENLÄ°KTE FALSE NEGATIVE KRÄ°TÄ°K UYARI âš ï¸
    # False Negative (saldÄ±rÄ±yÄ± kaÃ§Ä±rmak) en tehlikeli durumdur.
    # Ã‡Ã¼nkÃ¼ tespit edilemeyen bir saldÄ±rgan aÄŸda serbestÃ§e hareket eder,
    # veri Ã§alar veya sisteme kalÄ±cÄ± zarar verir.
    # IDS sistemlerinde FN oranÄ±nÄ±n sÄ±fÄ±ra yakÄ±n olmasÄ± hedeflenir.
    # Bu yÃ¼zden Recall (=TP/(TP+FN)) en Ã¶nemli metriktir.
    if fn > 0:
        print(f"\n  âš ï¸ UYARI: {fn} saldÄ±rÄ± kaÃ§Ä±rÄ±ldÄ± (False Negative)!")
        print(f"  Siber gÃ¼venlikte FN oranÄ±nÄ±n dÃ¼ÅŸÃ¼k olmasÄ± KRÄ°TÄ°KTÄ°R.")
        print(f"  KaÃ§Ä±rÄ±lan her saldÄ±rÄ± = Veri ihlali riski!")
    else:
        print(f"\n  âœ… MÃ¼kemmel! HiÃ§bir saldÄ±rÄ± kaÃ§Ä±rÄ±lmadÄ± (FN=0).")

    # â”€â”€ ROC Curve â”€â”€
    roc_auc = plot_roc_curve(y_test, y_proba, 'Random Forest')
    print(f"\n  ROC AUC: {roc_auc:.4f}")

    # â”€â”€ Feature Importance â”€â”€
    plot_feature_importance(model, feature_names, 'Random Forest', top_n=min(20, len(feature_names)))

    # â”€â”€ Model Kaydetme â”€â”€
    models_dir = os.path.join(PROJECT_ROOT, 'models')
    os.makedirs(models_dir, exist_ok=True)
    model_path = os.path.join(models_dir, 'random_forest.pkl')
    joblib.dump(model, model_path)
    print(f"\n  [âœ“] Model kaydedildi: {model_path}")

    return model, {
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1': f1,
        'roc_auc': roc_auc,
        'cv_recall_mean': cv_scores.mean(),
    }


def train_isolation_forest(X_train, X_test, y_test):
    """
    Isolation Forest ile denetimsiz anomali tespiti.
    Supervised ile karÅŸÄ±laÅŸtÄ±rma amacÄ±yla eklendi.
    """
    print("\n" + "=" * 60)
    print("ğŸ”§ ISOLATION FOREST (DENETÄ°MSÄ°Z) KARÅILAÅTIRMA")
    print("=" * 60)

    # Isolation Forest â€” etiket gerektirmez, sadece anomalileri tespit eder
    iso_model = IsolationForest(
        n_estimators=200,
        contamination=0.3,   # Verinin tahmini %30'u anomali
        random_state=42,
        n_jobs=-1,
    )

    # Sadece eÄŸitim verisiyle fit et (etiket KULLANILMAZ)
    iso_model.fit(X_train)

    # Test verisinde tahmin: 1=normal, -1=anomali
    y_pred_iso = iso_model.predict(X_test)
    # -1=anomali â†’ 1=saldÄ±rÄ±, 1=normal â†’ 0=normal
    y_pred_binary = np.where(y_pred_iso == -1, 1, 0)

    acc = accuracy_score(y_test, y_pred_binary)
    prec = precision_score(y_test, y_pred_binary, zero_division=0)
    rec = recall_score(y_test, y_pred_binary, zero_division=0)
    f1 = f1_score(y_test, y_pred_binary, zero_division=0)

    print(f"  Accuracy:    {acc:.4f}")
    print(f"  Precision:   {prec:.4f}")
    print(f"  Recall:      {rec:.4f}")
    print(f"  F1-Score:    {f1:.4f}")

    cm = confusion_matrix(y_test, y_pred_binary)
    plot_confusion_matrix(y_test, y_pred_binary, 'Isolation Forest')

    return {
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'f1': f1,
    }


def main():
    """Ana eÄŸitim akÄ±ÅŸÄ±."""
    # 1. Veri yÃ¼kle ve Ã¶n iÅŸle
    df = load_data()
    X_train, X_test, y_train, y_test, scaler, feature_names = preprocess(df)

    # 2. Random Forest (Denetimli)
    rf_model, rf_results = train_random_forest(X_train, X_test, y_train, y_test, feature_names)

    # 3. Isolation Forest (Denetimsiz â€” karÅŸÄ±laÅŸtÄ±rma)
    iso_results = train_isolation_forest(X_train, X_test, y_test)

    # 4. KarÅŸÄ±laÅŸtÄ±rma tablosu
    print("\n" + "=" * 60)
    print("ğŸ“Š DENETÄ°MLÄ° vs DENETÄ°MSÄ°Z KARÅILAÅTIRMA")
    print("=" * 60)
    print(f"\n{'Metrik':<15} {'Random Forest':>15} {'Isolation Forest':>18}")
    print("â”€" * 50)
    for metric in ['accuracy', 'precision', 'recall', 'f1']:
        rf_val = rf_results[metric]
        iso_val = iso_results[metric]
        winner = 'â† â˜…' if rf_val > iso_val else ''
        print(f"{metric:<15} {rf_val:>14.4f} {iso_val:>17.4f} {winner}")

    print(f"\nâœ… SonuÃ§: NSL-KDD etiketli veri seti iÃ§in Denetimli Ã–ÄŸrenme")
    print(f"   (Random Forest) daha iyi performans gÃ¶sterir.")
    print(f"   Ã–zellikle RECALL metriÄŸinde (saldÄ±rÄ± kaÃ§Ä±rma oranÄ±)")
    print(f"   denetimli model Ã¼stÃ¼ndÃ¼r.")

    print("\n" + "=" * 60)
    print("âœ… EÄÄ°TÄ°M TAMAMLANDI!")
    print("=" * 60)

    return rf_results, iso_results


if __name__ == '__main__':
    main()
